{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aeef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ MAIN() ************\n",
    "\n",
    "def main():\n",
    "    name_set = grab_link_titles()\n",
    "    crawling()\n",
    "    manually_delete_data_from_df()\n",
    "    \n",
    "    \n",
    "# ************ GRAB_LINK_TITLES() ************\n",
    "    \n",
    "def grab_link_titles():\n",
    "    '''Function returns set with all names contained in overview pages on Perrypedia by letter.'''\n",
    "    print(\"************ GRAB_LINK_TITLES() ************\")\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \n",
    "               \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "    name_set = set([])\n",
    "    for letter in letters:\n",
    "        print(f'Buchstabe: {letter}')\n",
    "        url = generate_URLs(letter)\n",
    "        page = get_page(url)\n",
    "        if page:\n",
    "            titles_by_page = get_link_titles(page)\n",
    "            for title in titles_by_page:\n",
    "                name_set.add(title)\n",
    "    print(\"Checking for bad data\")\n",
    "    name_set, bad_data = check_for_bad_data(name_set, letters)\n",
    "    with open(\"PR_person_namelist.txt\", \"w\") as file:\n",
    "        print(\"Saving as json\")\n",
    "        json.dump(list(name_set), file)\n",
    "    with open(\"bad_data.txt\", \"w\") as file:\n",
    "        json.dump(list(bad_data), file)\n",
    "    return name_set\n",
    "\n",
    "def generate_URLs(letter):\n",
    "    '''Function generates URL to overview page on Perrypedia for given letter.'''\n",
    "    root = \"https://www.perrypedia.de/wiki/Personen_\"\n",
    "    return root + letter\n",
    "\n",
    "\n",
    "def get_page(url):\n",
    "    '''Function returns page content for given URL as a parseable BeautifulSoup object.'''\n",
    "    page = requests.get(url)\n",
    "    if page.status_code != 200:\n",
    "        print(f\"{url}: {page.status_code}\")\n",
    "        soup = None\n",
    "    else:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_link_titles(soup):\n",
    "    '''Function searches given overview site as BeautifulSoup object for link titles containing character names.'''\n",
    "    titles = set([])\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        trs = table.find_all(\"tr\")\n",
    "        for row in trs:\n",
    "            page_content = row.find(\"td\")\n",
    "            if page_content != None:\n",
    "                link = page_content.find(\"a\")\n",
    "                if link != None:\n",
    "                    title = link.get(\"title\")\n",
    "                    if title != None and \"Seite nicht vorhanden\" not in title:\n",
    "                        titles.add(title)\n",
    "    return titles\n",
    "\n",
    "\n",
    "def check_for_bad_data(name_set, letters):\n",
    "    '''Function finds links that refer back to https://www.perrypedia.de/wiki/Personen_XYZ.'''\n",
    "    bad_data = set([])\n",
    "    for letter in letters:\n",
    "        url = f\"https://www.perrypedia.de/mediawiki/index.php?title=Spezial:Linkliste/Personen_{letter}&limit=1000&hidelinks=1\"\n",
    "        page = get_page(url)\n",
    "        if page:\n",
    "            name_set, doublettes = delete_doublettes(page, name_set)\n",
    "            bad_data.update(doublettes)\n",
    "    return name_set, bad_data\n",
    "\n",
    "\n",
    "def delete_doublettes(page, name_set):\n",
    "    '''Function deletes doublettes from name_set.'''\n",
    "    bad_data = set([])\n",
    "    name_list = page.find(\"ul\")\n",
    "    links = name_list.find_all(\"a\")\n",
    "    for link in links:\n",
    "        title = link.get(\"title\")\n",
    "        if title in name_set:\n",
    "            bad_data.add(title)\n",
    "            name_set.remove(title)\n",
    "    return name_set, bad_data\n",
    "\n",
    "\n",
    "# ************ CRAWLING() ************\n",
    "\n",
    "def crawling():\n",
    "    '''Function iterates through name set generating a Soup object for the corresponding URL and saves results of crawling as adjacency matrix and as URL list.'''\n",
    "    print(\"************ CRAWLING() ************\")\n",
    "    print(\"Preparing data\")\n",
    "    name_list, name_set, length = data_preparation()\n",
    "    global df\n",
    "    df = pd.DataFrame(columns=name_set, index=name_set)\n",
    "    i = 0\n",
    "    url_mapping = {}\n",
    "    for name in name_list:\n",
    "            url = \"https://www.perrypedia.de/wiki/\" + name.replace(\" \", \"_\")\n",
    "            url_mapping[name] = url\n",
    "            try:\n",
    "                page = get_page(url)\n",
    "                if page:\n",
    "                    crawl_for_names_in_text(page, name, name_set)\n",
    "            except:\n",
    "                print(f\"Fehler bei {i}: {name}\")\n",
    "            finally:\n",
    "                i += 1\n",
    "                if i%100==0:\n",
    "                  print(f\"{i} / {length} pages processed.\")\n",
    "    print(\"Saving as .csv\")\n",
    "    df.to_csv(\"PR_person_matrix.csv\")\n",
    "    with open(\"url_mapping.json\", \"w\") as file:\n",
    "        json.dump(url_mapping, file)\n",
    "    print(\"Crawling finished.\")\n",
    "    \n",
    "\n",
    "def data_preparation():\n",
    "    '''Function loads and returns the namelist and its length from file.'''\n",
    "    with open(\"./PR_person_namelist.txt\", \"r\") as file:\n",
    "        name_list = json.load(file)\n",
    "    name_set = set(name_list)\n",
    "    length = len(name_set)\n",
    "    return name_list, name_set, length\n",
    "\n",
    "\n",
    "def crawl_for_names_in_text(page, name, name_set):\n",
    "    '''Function crawls for link titles for a given page, matches them with corresponding names in the namelist and manages entry in adjacency matrix'''\n",
    "    global df\n",
    "    links = page.find_all(\"a\")\n",
    "    if links:\n",
    "        for link in links:\n",
    "            title = link.get(\"title\")\n",
    "            if title in name_set and title is not name:\n",
    "                # df[name][title] = 1\n",
    "                df[title][name] = 1\n",
    "            \n",
    "\n",
    "# ************ MANUALLY_DELETE_DATA_FROM_DF() ************            \n",
    "            \n",
    "def manually_delete_data_from_df():\n",
    "    '''Function deletes bad data given in bad_data.txt from adjacency matrix.'''\n",
    "    df = pd.read_csv(\"PR_person_matrix.csv\", sep = \",\", index_col=\"00\")\n",
    "    print(\"Finished loading df\")\n",
    "    with open(\"bad_data.txt\", \"r\") as file:\n",
    "        bad_data = json.load(file)\n",
    "    for name in bad_data:\n",
    "        df.drop(labels=name, axis=\"index\")\n",
    "        df.drop(labels=name, axis=\"columns\")\n",
    "    print(\"Saving to csv\")\n",
    "    df.to_csv(\"PR_person_matrix_clean.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d8381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
